{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kzkedzierska/miniconda3/lib/python3.6/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n",
      "/home/kzkedzierska/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# General modules\n",
    "import os\n",
    "\n",
    "# Data wrangling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "import zegamiML\n",
    "        \n",
    "# Dimensionality reduction\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.manifold import MDS, TSNE, Isomap\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.preprocessing import scale\n",
    "import umap\n",
    "\n",
    "# Clustering\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.cluster import KMeans, MeanShift, DBSCAN\n",
    "import hdbscan\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Plotting\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST file reader\n",
    "\"\"\" A function that can read MNIST's idx file format into numpy arrays.\n",
    "    The MNIST data files can be downloaded from here:\n",
    "    \n",
    "    http://yann.lecun.com/exdb/mnist/\n",
    "    This relies on the fact that the MNIST dataset consistently uses\n",
    "    unsigned char types with their data segments.\n",
    "\"\"\"\n",
    "\n",
    "import struct\n",
    "\n",
    "def read_idx(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.fromstring(f.read(), dtype=np.uint8).reshape(shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kzkedzierska/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "raw_train = read_idx(\"./mnist/train-images-idx3-ubyte\")\n",
    "train_data = np.reshape(raw_train, (60000,28*28))\n",
    "train_labels =  read_idx(\"./mnist/train-labels-idx1-ubyte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chosen_number = 7\n",
    "MNIST_data = train_data #[train_labels == chosen_number]\n",
    "MNIST_dt = pd.DataFrame({'number': train_labels}) #[train_labels == chosen_number]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BullsEye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>disease</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3943.586594</td>\n",
       "      <td>-2378.194352</td>\n",
       "      <td>CAD</td>\n",
       "      <td>64_BullsEye.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1617.265459</td>\n",
       "      <td>-339.951244</td>\n",
       "      <td>CAD</td>\n",
       "      <td>20_BullsEye.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3774.299893</td>\n",
       "      <td>-1465.415170</td>\n",
       "      <td>CAD</td>\n",
       "      <td>17_BullsEye.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2182.320656</td>\n",
       "      <td>-1604.897623</td>\n",
       "      <td>CAD</td>\n",
       "      <td>96_BullsEye.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6509.727480</td>\n",
       "      <td>1997.538891</td>\n",
       "      <td>CAD</td>\n",
       "      <td>50_BullsEye.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id            x            y disease             path\n",
       "0   0  3943.586594 -2378.194352     CAD  64_BullsEye.png\n",
       "1   1  1617.265459  -339.951244     CAD  20_BullsEye.png\n",
       "2   2  3774.299893 -1465.415170     CAD  17_BullsEye.png\n",
       "3   3  2182.320656 -1604.897623     CAD  96_BullsEye.png\n",
       "4   4  6509.727480  1997.538891     CAD  50_BullsEye.png"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zt = pd.read_csv(\"./test_data/zegami.tab\", sep = \"\\t\", header = 0)\n",
    "zt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>127.000000</td>\n",
       "      <td>1.270000e+02</td>\n",
       "      <td>1.270000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>63.000000</td>\n",
       "      <td>-4.724412e-08</td>\n",
       "      <td>-7.873966e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>36.805797</td>\n",
       "      <td>3.473482e+03</td>\n",
       "      <td>2.376548e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.064702e+03</td>\n",
       "      <td>-5.990448e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>31.500000</td>\n",
       "      <td>-3.020156e+03</td>\n",
       "      <td>-1.290637e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>63.000000</td>\n",
       "      <td>-8.954199e+02</td>\n",
       "      <td>3.123476e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>94.500000</td>\n",
       "      <td>2.303134e+03</td>\n",
       "      <td>1.577495e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>126.000000</td>\n",
       "      <td>9.040996e+03</td>\n",
       "      <td>6.131664e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id             x             y\n",
       "count  127.000000  1.270000e+02  1.270000e+02\n",
       "mean    63.000000 -4.724412e-08 -7.873966e-09\n",
       "std     36.805797  3.473482e+03  2.376548e+03\n",
       "min      0.000000 -5.064702e+03 -5.990448e+03\n",
       "25%     31.500000 -3.020156e+03 -1.290637e+03\n",
       "50%     63.000000 -8.954199e+02  3.123476e+02\n",
       "75%     94.500000  2.303134e+03  1.577495e+03\n",
       "max    126.000000  9.040996e+03  6.131664e+03"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zt.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./test_data/out/\"\n",
    "image_col = 'path'\n",
    "\n",
    "images = [os.path.join(image_path, image) for image in zt[image_col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, skipped = zegamiML.process_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127, 30000)\n",
      "(127, 20038)\n"
     ]
    }
   ],
   "source": [
    "# remove columns with variance = 0\n",
    "df = pd.DataFrame(data)\n",
    "print(df.shape)\n",
    "df = df.loc[:, ~np.isclose(0, df.var())]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_df = pd.concat([zt, pd.DataFrame(data)], axis = 1)\n",
    "# full_df.to_csv(\"./full_df.tsv\", sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical introduction to dimensionality reduction \n",
    "\n",
    "## Definitions\n",
    "\n",
    "**Sparse matrix**\n",
    "\n",
    "Sparse matrix is a matrix in which most elements are zero (i.e. do not carry information). By contrast, if most of the elements are nonzero, then the matrix is considered dense. [Wikipedia](https://en.wikipedia.org/wiki/Sparse_matrix)\n",
    "\n",
    "## Data preparation\n",
    "\n",
    "When the multidimensional data is coming from different experiments or has a different origin it is necessary to pre-process it so it can be properly analyzed. It might be needed to transform the numeric values to some common scale in order to make comparisons meaningful. Centering means subtracting the mean, so that the mean of the centered data is at the origin. Scaling or standardizing then means dividing by the standard deviation, so that the new standard deviation is 1. \n",
    "\n",
    "The aim of log and asinh transformations is (usually) **variance stabilization**, i.e., to make the variances of replicate measurements of one and the same variable in different parts of its dynamic range more similar. In contrast, the standardizing transformation aims to make the **scale** (as measured by mean and standard deviation) of different variables **the same**.\n",
    "\n",
    "\n",
    "In brief, dimensionality reduction is a task of representing multidimensional data (for example single cell expression data) into  low dimensional (1-3D) space.\n",
    "  \n",
    "![](https://imgs.xkcd.com/comics/artifacts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical / Metric Multidimensioanl Scaling == Principal Coordinate Analysis (PCoA)\n",
    "\n",
    "**Note** sklearn implementation uses the non-metric MDS, conversely in R the default MDS is a classical/metric MDS!\n",
    "\n",
    "[StatQQuest with Josh Starmer](https://youtu.be/GEn-_dAyYME)\n",
    "    \n",
    "Very similar to PCA, instead of converting correlations into a 2-D graph PCoA converts distances among the samples into a 2-D graph. \n",
    "\n",
    "Minimizing the linear distances is the same as maximizing the linear correlations, i.e. PCoA on Euclidean distances is equivalent to PCA on 2 components.\n",
    "\n",
    "1. Step 1. Calculate pairwise distances between samples\n",
    "1. Step 2. Eigen Decomposition\n",
    "    \n",
    "Euclidean distance:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + ... + (x_n - y_n)^2}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear methods\n",
    "\n",
    "### Non-Metric Multidimensional Scaling (NMDS)\n",
    "\n",
    "Having Multivariate data and specified distances in the multidimensional space NMDS tries to find a Euclidean distance matrix in 2D that will best correlate with the distances in the original space. Good alternative for data were linear approaches are not suitable (particularly counts of abundance). The initial distance can be specifically chosen for particular data.\n",
    "\n",
    "Goodness-of-fit is measured by \"stress\" - a measure of rank-order disagreement between observed and fitted distances.\n",
    "\n",
    "Goodness of fit rules of thumb:\n",
    "* \\> 0.2 Poor - risk in interpretation\n",
    "* 0.1 - 0.2 Fair - some distances might be misleading\n",
    "* 0.05 - 0.1 Good - inferences confident\n",
    "* < 0.05 Excellent\n",
    "\n",
    "[Matthew E. Clapham](https://youtu.be/Kl49qI3XJKY)\n",
    "\n",
    "Arranges points to maximize rank-order correlation between real-world distance and ordination space distance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tSNE - t-distributed Stochastic Neighbor Embedding\n",
    "\n",
    "1. Step 1. Determining the similarity of the points.\n",
    "\n",
    "    For each point a normal curve centered on that point is constructed. The width of this curve is based on the density of the surrounding data points which is related to the perplexity argument (i.e. the bigger the perplexity the wider the curve, more points are included in estimating the density).  Then, unscaled similarity is expressed as the height of that normal curve within the distance to each of the other points. Next, for each point the similarities are scaled to add up to 1. Because the curve is constructed for each point separately, the AB and BA distances might be different, therefore the algorithm averages them out.\n",
    "\n",
    "    Similarity between point A and A is expressed as 0. \n",
    "\n",
    "1. Step 2. \n",
    "\n",
    "    Next, the points are randomly projected on the chosen dimensions (line in 1D, plane in 2D and space in 3D). Like in the step 1, the similarities are calculated by measuring the height of the distribution curve. Only this time, the curve is a t-distribution curve.\n",
    "    \n",
    "    Now, there are two similarities matrices - one from Step 1 (D1, based on normal distribution)  and one from this Step (D2, based on t-distribution). \n",
    "\n",
    "1. Step 3. Moving the points \n",
    "\n",
    "    The positions of the points on the projection space are changed so the matrix D2 resembles matrix D1. Points are move one at the time. The D2 is recalculated after each move (?). \n",
    "\n",
    "#### Perplexity\n",
    "\n",
    "perplexity - According to the text on [\"How to Use t-SNE Effectively\"](https://distill.pub/2016/misread-tsne/): \n",
    "\n",
    "<img src=https://scikit-learn.org/stable/_images/sphx_glr_plot_t_sne_perplexity_001.png width=\"700\">\n",
    "\n",
    "t-SNE can be potentially used if we use a non-distance based clustering techniques like FMM (Finite Mixture Models) or DBSCAN (Density-based Models). As you correctly note, in such cases, the t-SNE output can quite helpful. The issue in these use cases is that some people might try to read into the cluster placement and not only the cluster membership. As the global distances are lost, drawing conclusions from cluster placement can lead to bogus insights. Notice that just saying: \"hey, we found all the 1s cluster together\" does not offer great value if cannot say what they are far from. If we just wanted to find the 1's we might as well have used classification to begin with (which bring us back to the use of autoencoders). from [here](https://stats.stackexchange.com/a/340185)\n",
    "\n",
    "#### What to watch out for?\n",
    "\n",
    "[How to Use t-Sne Effectively](https://distill.pub/2016/misread-tsne/)\n",
    "\n",
    "1. Perlexity\n",
    " \n",
    "    The parameter is, in a sense, a guess about the number of close neighbors each point has. It **can not be greater than number of points**. However, the exact number is not that easy to know. Authors recommend a number between 2 and 50. The smaller the number the more local variations will dominate.\n",
    "    \n",
    "1. Number of steps\n",
    "    \n",
    "    Number of iterations might influence the stability of the projection, however the number of steps to reach the stability is data dependent. \n",
    "    \n",
    "1. The distances are (usually) not meaningful\n",
    "\n",
    "    The spread of the cluster on the tSNE plot is not meaningful of it's spread in the original space. The same is true for distances between particular clusters. It might give a good sense of global geometry, however that greatly depends on data and the parameters chosen (mostly fine-tunning perplexity).\n",
    "    \n",
    "1. t-SNE tends to expand denser regions of data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP\n",
    "\n",
    "[UMAP @ SciPy2018](https://www.youtube.com/watch?v=nq6iPZVUxZU)\n",
    "\n",
    "From what I understand from the maths behind the method (unfortunately that's not much :D) is that UMAP, besides using the attractive force (as does tSNE) it uses the repulsive force as well.\n",
    "\n",
    "UMAP has several hyperparameters that can have a significant impact on the resulting embedding. In this notebook we will be covering the four major ones:\n",
    "\n",
    "* n_neighbors  \n",
    "    This paramater is responsible for balancing the local vs global structure. Low values will force UMAP to focus on local structure, while large values will mostly preserve the global structure.\n",
    "* min_dist  \n",
    "    This parameter controls how tightly UMAP will pack the data points. I.e. the smaller the value of min_dist, the clumpier the plot will look like.\n",
    "* n_components  \n",
    "    UMAP scales well to more than 3D. \n",
    "* metric  \n",
    "    This controls the distance metric. There are various ones implemented within the umap algorithm, however user can supply custom distance matrix.  \n",
    "    Supported distance metrics:\n",
    "    * Minkowski style metrics\n",
    "        * euclidean\n",
    "        * manhattan\n",
    "        * chebyshev\n",
    "        * minkowski\n",
    "    * Miscellaneous spatial metrics\n",
    "        * canberra\n",
    "        * braycurtis\n",
    "        * haversine\n",
    "    * Normalized spatial metrics\n",
    "        * mahalanobis\n",
    "        * wminkowski\n",
    "        * seuclidean\n",
    "    * Angular and correlation metrics\n",
    "        * cosine\n",
    "        * correlation\n",
    "    * Metrics for binary data\n",
    "        * hamming\n",
    "        * jaccard\n",
    "        * dice\n",
    "        * russellrao\n",
    "        * kulsinski\n",
    "        * rogerstanimoto\n",
    "        * sokalmichener\n",
    "        * sokalsneath\n",
    "        * yule\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering \n",
    "\n",
    "[Twitter discussion](https://twitter.com/leland_mcinnes/status/1014968417697267712)  \n",
    "[StackExchange discussion](https://stats.stackexchange.com/questions/263539/clustering-on-the-output-of-t-sne)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_types = ['PCA', 'MDS', 'tSNE', 'umap', \"Isomap\"]\n",
    "\n",
    "def plot_all(df_with_coord, labels = None, types = analysis_types, marker_size = 0.25):\n",
    "    fig = plt.figure(figsize = (25, 5))\n",
    "    for i in range(0, len(types)):\n",
    "        analysis_type = types[i]\n",
    "        # plotting\n",
    "        plt.subplot(1, len(types), i + 1)\n",
    "        if labels is not None:\n",
    "            plt.scatter(df_with_coord['x_' + analysis_type], \n",
    "                        df_with_coord['y_' + analysis_type], \n",
    "                        c = labels, s = marker_size)\n",
    "        else:\n",
    "            plt.scatter(df_with_coord['x_' + analysis_type], \n",
    "                        df_with_coord['y_' + analysis_type],\n",
    "                        marker_size)\n",
    "        plt.title(analysis_type)\n",
    "    return fig\n",
    "\n",
    "def reduce_dim(data, zt_in, types = analysis_types):\n",
    "    zt_out = zt_in\n",
    "    for i in range(0, len(types)):\n",
    "        analysis_type = types[i]\n",
    "\n",
    "        # reduce dimentions\n",
    "        reduced_df = zegamiML.reduce_dimensions(data, \n",
    "                                                analysis_type = analysis_type)\n",
    "        \n",
    "        # merge input and reduced data frames\n",
    "        reduced_df = reduced_df.rename(index = str, \n",
    "                                       columns = {'x': 'x_' + analysis_type, \n",
    "                                                  'y': 'y_' + analysis_type})\n",
    "        reduced_df[image_col] = [os.path.basename(image_fp) for image_fp in images\n",
    "                                 if image_fp not in skipped]\n",
    "        \n",
    "        \n",
    "        zt_out = pd.merge(zt_out, reduced_df, on = image_col, how = 'outer')\n",
    "    \n",
    "    return zt_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zt_out = reduce_dim(df, zt)\n",
    "disease_labels = pd.factorize(zt_out['disease'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supported_types = {\n",
    "        \"PCA\" : PCA(n_components = 2),\n",
    "        \"TSNE\" : TSNE(init = 'pca'), \n",
    "        \"UMAP\" : umap.UMAP(), \n",
    "        \"MDS\" : MDS(n_components = 2),\n",
    "        \"ISOMAP\": Isomap(n_components = 2)\n",
    "    }\n",
    "\n",
    "for typ in [\"PCA\", \"UMAP\", \"MDS\"]:\n",
    "    typ = typ.upper()\n",
    "\n",
    "    reducer = supported_types[typ]\n",
    "\n",
    "    X = reducer.fit_transform(MNIST_data)\n",
    "    MNIST_dt['x_' + typ] = X[:, 0]\n",
    "    MNIST_dt['y_' + typ] = X[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plot_all(MNIST_dt, types = [\"PCA\", \"UMAP\"], labels = train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zt_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figur = plot_all(zt_out, labels = disease_labels, marker_size = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality of DR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_pcs(evr, ratio = 0.7, plot = False, PCs = 0):\n",
    "    cs_evr = np.cumsum(evr)\n",
    "    pcs_expl = min([i[0] for i, x in np.ndenumerate(cs_evr) if x > ratio]) + 1\n",
    "    if PCs == 0:\n",
    "        PCs = pcs_expl\n",
    "    if plot:\n",
    "        var_explained = round(cs_evr[PCs-1]*100, 0)\n",
    "        plt.bar(range(1, len(evr[0:PCs])+1), \n",
    "                evr[0:PCs] * 100)\n",
    "        plt.title(f'Scree plot\\nVariance explained: {var_explained}%')\n",
    "        plt.ylabel('% of variance explained')\n",
    "        plt.xlabel('Principal Component')\n",
    "        plt.xticks(range(1, len(evr[0:PCs])+1))\n",
    "    return pcs_expl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit_transform(df)\n",
    "\n",
    "pcs = n_pcs(pca.explained_variance_ratio_, plot = True, PCs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt = 0.5\n",
    "pcs = n_pcs(pca.explained_variance_ratio_, plot = True, ratio = rt)\n",
    "\n",
    "print(f'{rt*100}% of variance is explained by {pcs} PCs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_1d_to_3d(method = \"PCA\", data = df, labels = None):\n",
    "    method = method.upper()\n",
    "    methods = [\"PCA\", \"TSNE\", \"UMAP\"]\n",
    "    if method not in methods:\n",
    "        raise ValueError(f'{method} not suppoerted!')\n",
    "    figr = plt.figure(figsize = (15, 5))\n",
    "    for i in range(0, 3):\n",
    "        if method == \"PCA\":\n",
    "            reducer = PCA(n_components = i+1)\n",
    "        elif method == \"TSNE\":\n",
    "            reducer = TSNE(n_components = i+1)\n",
    "        else: # method == \"UMAP\"\n",
    "            reducer = umap.UMAP(n_components = i+1)\n",
    "        X = reducer.fit_transform(data)\n",
    "        if i == 0:\n",
    "            ax = plt.subplot(1, 3, i + 1)\n",
    "            if labels is not None:\n",
    "                ax.scatter(X[:,0], range(len(X)), \n",
    "                           c = labels)\n",
    "            else:\n",
    "                ax.scatter(X[:,0], range(len(X)))\n",
    "        elif i == 1:\n",
    "            ax = plt.subplot(1, 3, i + 1)\n",
    "            if labels is not None:\n",
    "                ax.scatter(X[:,0], X[:,1], \n",
    "                           c = labels)\n",
    "            else:\n",
    "                ax.scatter(X[:,0], X[:,1])\n",
    "        else:\n",
    "            ax = plt.subplot(1, 3, i + 1, projection = '3d')\n",
    "            if labels is not None:\n",
    "                ax.scatter(X[:,0], X[:,1], X[:,2], \n",
    "                           s = 100,\n",
    "                           c = labels)\n",
    "            else:\n",
    "                ax.scatter(X[:,0], X[:,1], X[:,2], \n",
    "                           s = 100)\n",
    "        plt.title(f'{i+1}D')\n",
    "    figr.suptitle(method)\n",
    "    figr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_1d_to_3d(\"PCA\", labels = disease_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_1d_to_3d(\"tSNE\", labels = disease_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_1d_to_3d(\"umap\", labels = disease_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 2, random_state = 7).fit(df)\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 2, random_state = 7).fit(zt_out[[\"x_umap\", \"y_umap\"]])\n",
    "labels = kmeans.labels_\n",
    "\n",
    "\n",
    "f = plot_all(zt_out, labels = disease_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zt_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections.Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figur2 = plot_all(zt_out, labels = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimal k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sum_of_squared_distances = []\n",
    "K = range(1,15)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters = k)\n",
    "    km = km.fit(data)\n",
    "    Sum_of_squared_distances.append(km.inertia_)\n",
    "    \n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimalK(data, nrefs=3, maxClusters=15):\n",
    "    # copied from: https://anaconda.org/milesgranger/gap-statistic/notebook\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates KMeans optimal K using Gap Statistic from Tibshirani, Walther, Hastie\n",
    "    Params:\n",
    "        data: ndarry of shape (n_samples, n_features)\n",
    "        nrefs: number of sample reference datasets to create\n",
    "        maxClusters: Maximum number of clusters to test for\n",
    "    Returns: (gaps, optimalK)\n",
    "    \"\"\"\n",
    "    gaps = np.zeros((len(range(1, maxClusters)),))\n",
    "    resultsdf = pd.DataFrame({'clusterCount':[], 'gap':[]})\n",
    "    for gap_index, k in enumerate(range(1, maxClusters)):\n",
    "\n",
    "        # Holder for reference dispersion results\n",
    "        refDisps = np.zeros(nrefs)\n",
    "\n",
    "        # For n references, generate random sample and perform kmeans getting resulting dispersion of each loop\n",
    "        for i in range(nrefs):\n",
    "            \n",
    "            # Create new random reference set\n",
    "            randomReference = np.random.random_sample(size=data.shape)\n",
    "            \n",
    "            # Fit to it\n",
    "            km = KMeans(k)\n",
    "            km.fit(randomReference)\n",
    "            \n",
    "            refDisp = km.inertia_\n",
    "            refDisps[i] = refDisp\n",
    "\n",
    "        # Fit cluster to original data and create dispersion\n",
    "        km = KMeans(k)\n",
    "        km.fit(data)\n",
    "        \n",
    "        origDisp = km.inertia_\n",
    "\n",
    "        # Calculate gap statistic\n",
    "        gap = np.log(np.mean(refDisps)) - np.log(origDisp)\n",
    "\n",
    "        # Assign this loop's gap statistic to gaps\n",
    "        gaps[gap_index] = gap\n",
    "        \n",
    "        resultsdf = resultsdf.append({'clusterCount':k, 'gap':gap}, ignore_index=True)\n",
    "\n",
    "    return (gaps.argmax() + 1, resultsdf)  # Plus 1 because index of 0 means 1 cluster is optimal, index 2 = 3 clusters are optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimalK(np.array(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MeanShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = MeanShift(bandwidth = 50).fit(data)\n",
    "labels = clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collections.Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figur3 = plot_all(zt_out, labels = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Density-Based Spatial Clustering of Applications with Noise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = DBSCAN(eps = 10000, min_samples = 2).fit(data)\n",
    "labels = clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections.Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figur4 = plot_all(zt_out, labels = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_data = distance_matrix(data, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.hstack(dist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(a, bins = 'auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_labels = hdbscan.HDBSCAN(core_dist_n_jobs = 24).fit_predict(MNIST_data)\n",
    "collections.Counter(hdbscan_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_labels = hdbscan.HDBSCAN(min_samples=2, min_cluster_size=500).fit_predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections.Counter(hdbscan_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(zt['x'], zt['y'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
